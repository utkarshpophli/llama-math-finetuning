{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:46:00.150744Z","iopub.status.busy":"2024-07-07T10:46:00.150402Z","iopub.status.idle":"2024-07-07T10:46:28.001235Z","shell.execute_reply":"2024-07-07T10:46:28.000010Z","shell.execute_reply.started":"2024-07-07T10:46:00.150718Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Config Hyperparameter"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:28:56.972705Z","iopub.status.busy":"2024-07-07T08:28:56.972314Z","iopub.status.idle":"2024-07-07T08:28:56.980330Z","shell.execute_reply":"2024-07-07T08:28:56.979259Z","shell.execute_reply.started":"2024-07-07T08:28:56.972680Z"},"trusted":true},"outputs":[],"source":["model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","dataset_name = \"utkarshpophli/mathematics_dataset\"\n","\n","new_model = \"llama-2-7b-math-finetune\"\n","\n","# QLoRA parameters\n","lora_r = 64\n","lora_alpha = 16\n","lora_dropout = 0.1\n","\n","# bitsandbytes parameters\n","use_4bit = True\n","bnb_4bit_compute_dtype = \"float16\"\n","bnb_4bit_quant_type = \"nf4\"\n","use_nested_quant = False\n","output_dir = \"./results\"\n","num_train_epochs = 1\n","fp16 = False\n","bf16 = False\n","per_device_train_batch_size = 4\n","per_device_eval_batch_size = 4\n","gradient_accumulation_steps = 1\n","gradient_checkpointing = True\n","max_grad_norm = 0.3\n","learning_rate = 2e-4\n","weight_decay = 0.001\n","optim = \"paged_adamw_32bit\"\n","lr_scheduler_type = \"cosine\"\n","max_steps = -1\n","warmup_ratio = 0.03\n","group_by_length = True\n","save_steps = 0\n","logging_steps = 25\n","\n","# SFT parameters\n","max_seq_length = None\n","packing = False\n","device_map = {\"\": 0}"]},{"cell_type":"markdown","metadata":{},"source":["openai/gsm8k"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:42:12.396408Z","iopub.status.busy":"2024-07-07T08:42:12.396014Z","iopub.status.idle":"2024-07-07T08:42:13.797022Z","shell.execute_reply":"2024-07-07T08:42:13.796081Z","shell.execute_reply.started":"2024-07-07T08:42:12.396373Z"},"trusted":true},"outputs":[],"source":["# Load dataset\n","dataset = load_dataset(dataset_name, split=\"train[:20%]\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:42:15.500948Z","iopub.status.busy":"2024-07-07T08:42:15.500093Z","iopub.status.idle":"2024-07-07T10:24:09.593261Z","shell.execute_reply":"2024-07-07T10:24:09.592392Z","shell.execute_reply.started":"2024-07-07T08:42:15.500913Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ceee480c276a420c98137e8c610da69c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a69842d9feb44b30bda957c36f484d64","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/17432 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4358' max='4358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4358/4358 1:41:20, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>3.172000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>5.808300</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>2.712100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>3.853700</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.868000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>3.240900</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.649100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>3.186500</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.604900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>2.925700</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.723800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.654700</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.505600</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>3.031700</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.458900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.169500</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.481900</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>2.363000</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.410700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>3.197700</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.558600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>2.158400</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.333300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>2.546900</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>1.514300</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.849800</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.356900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>3.035700</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.514300</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>2.238100</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.330900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>2.218800</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.439200</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.896900</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.354800</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.879900</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.444300</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.739400</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.388500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.010800</td>\n","    </tr>\n","    <tr>\n","      <td>1025</td>\n","      <td>1.402800</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>1.608500</td>\n","    </tr>\n","    <tr>\n","      <td>1075</td>\n","      <td>1.398800</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>1.924100</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>1.392900</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>2.039200</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>1.320500</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.472400</td>\n","    </tr>\n","    <tr>\n","      <td>1225</td>\n","      <td>1.386200</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>2.328700</td>\n","    </tr>\n","    <tr>\n","      <td>1275</td>\n","      <td>1.442300</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>1.762100</td>\n","    </tr>\n","    <tr>\n","      <td>1325</td>\n","      <td>1.363900</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>1.689000</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>1.326400</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.946200</td>\n","    </tr>\n","    <tr>\n","      <td>1425</td>\n","      <td>1.281300</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>1.958000</td>\n","    </tr>\n","    <tr>\n","      <td>1475</td>\n","      <td>1.384500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.798400</td>\n","    </tr>\n","    <tr>\n","      <td>1525</td>\n","      <td>1.353400</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>2.075000</td>\n","    </tr>\n","    <tr>\n","      <td>1575</td>\n","      <td>1.336800</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>2.120900</td>\n","    </tr>\n","    <tr>\n","      <td>1625</td>\n","      <td>1.386900</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>1.947100</td>\n","    </tr>\n","    <tr>\n","      <td>1675</td>\n","      <td>1.249500</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>2.175500</td>\n","    </tr>\n","    <tr>\n","      <td>1725</td>\n","      <td>1.308200</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>1.714500</td>\n","    </tr>\n","    <tr>\n","      <td>1775</td>\n","      <td>1.257900</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>1.964100</td>\n","    </tr>\n","    <tr>\n","      <td>1825</td>\n","      <td>1.346700</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>1.968300</td>\n","    </tr>\n","    <tr>\n","      <td>1875</td>\n","      <td>1.334200</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>1.651500</td>\n","    </tr>\n","    <tr>\n","      <td>1925</td>\n","      <td>1.342200</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>1.467400</td>\n","    </tr>\n","    <tr>\n","      <td>1975</td>\n","      <td>1.201700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.167000</td>\n","    </tr>\n","    <tr>\n","      <td>2025</td>\n","      <td>1.253900</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>1.620300</td>\n","    </tr>\n","    <tr>\n","      <td>2075</td>\n","      <td>1.173500</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>2.013800</td>\n","    </tr>\n","    <tr>\n","      <td>2125</td>\n","      <td>1.260600</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>1.717400</td>\n","    </tr>\n","    <tr>\n","      <td>2175</td>\n","      <td>1.255200</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>1.681300</td>\n","    </tr>\n","    <tr>\n","      <td>2225</td>\n","      <td>1.149600</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>2.212800</td>\n","    </tr>\n","    <tr>\n","      <td>2275</td>\n","      <td>1.292900</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>1.693100</td>\n","    </tr>\n","    <tr>\n","      <td>2325</td>\n","      <td>1.418300</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>1.983700</td>\n","    </tr>\n","    <tr>\n","      <td>2375</td>\n","      <td>1.328700</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>1.824700</td>\n","    </tr>\n","    <tr>\n","      <td>2425</td>\n","      <td>1.140500</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>1.970500</td>\n","    </tr>\n","    <tr>\n","      <td>2475</td>\n","      <td>1.215900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.572100</td>\n","    </tr>\n","    <tr>\n","      <td>2525</td>\n","      <td>1.241000</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>1.916900</td>\n","    </tr>\n","    <tr>\n","      <td>2575</td>\n","      <td>1.219300</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>1.611700</td>\n","    </tr>\n","    <tr>\n","      <td>2625</td>\n","      <td>1.242200</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>1.384500</td>\n","    </tr>\n","    <tr>\n","      <td>2675</td>\n","      <td>1.323000</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>1.657600</td>\n","    </tr>\n","    <tr>\n","      <td>2725</td>\n","      <td>1.233400</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>1.257800</td>\n","    </tr>\n","    <tr>\n","      <td>2775</td>\n","      <td>1.161100</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>2.071700</td>\n","    </tr>\n","    <tr>\n","      <td>2825</td>\n","      <td>1.178000</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>1.638900</td>\n","    </tr>\n","    <tr>\n","      <td>2875</td>\n","      <td>1.337200</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>1.647900</td>\n","    </tr>\n","    <tr>\n","      <td>2925</td>\n","      <td>1.287000</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>1.529000</td>\n","    </tr>\n","    <tr>\n","      <td>2975</td>\n","      <td>1.154400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.725500</td>\n","    </tr>\n","    <tr>\n","      <td>3025</td>\n","      <td>1.092500</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>1.625400</td>\n","    </tr>\n","    <tr>\n","      <td>3075</td>\n","      <td>1.200600</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>1.792600</td>\n","    </tr>\n","    <tr>\n","      <td>3125</td>\n","      <td>1.204400</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>1.612700</td>\n","    </tr>\n","    <tr>\n","      <td>3175</td>\n","      <td>1.214500</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>1.499200</td>\n","    </tr>\n","    <tr>\n","      <td>3225</td>\n","      <td>1.159400</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>1.634200</td>\n","    </tr>\n","    <tr>\n","      <td>3275</td>\n","      <td>1.258100</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>1.582100</td>\n","    </tr>\n","    <tr>\n","      <td>3325</td>\n","      <td>1.213500</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>1.667200</td>\n","    </tr>\n","    <tr>\n","      <td>3375</td>\n","      <td>1.235100</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>1.749300</td>\n","    </tr>\n","    <tr>\n","      <td>3425</td>\n","      <td>1.146000</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>1.778100</td>\n","    </tr>\n","    <tr>\n","      <td>3475</td>\n","      <td>1.298100</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.458100</td>\n","    </tr>\n","    <tr>\n","      <td>3525</td>\n","      <td>1.162300</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>1.358300</td>\n","    </tr>\n","    <tr>\n","      <td>3575</td>\n","      <td>1.225800</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>1.821000</td>\n","    </tr>\n","    <tr>\n","      <td>3625</td>\n","      <td>1.393000</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>1.733000</td>\n","    </tr>\n","    <tr>\n","      <td>3675</td>\n","      <td>1.342100</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>1.293700</td>\n","    </tr>\n","    <tr>\n","      <td>3725</td>\n","      <td>1.207800</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>1.545000</td>\n","    </tr>\n","    <tr>\n","      <td>3775</td>\n","      <td>1.296900</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>1.647400</td>\n","    </tr>\n","    <tr>\n","      <td>3825</td>\n","      <td>1.171000</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>1.764300</td>\n","    </tr>\n","    <tr>\n","      <td>3875</td>\n","      <td>0.988900</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>1.679500</td>\n","    </tr>\n","    <tr>\n","      <td>3925</td>\n","      <td>1.054000</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>1.462300</td>\n","    </tr>\n","    <tr>\n","      <td>3975</td>\n","      <td>1.203600</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.668900</td>\n","    </tr>\n","    <tr>\n","      <td>4025</td>\n","      <td>1.135800</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>1.424600</td>\n","    </tr>\n","    <tr>\n","      <td>4075</td>\n","      <td>1.190600</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>1.755900</td>\n","    </tr>\n","    <tr>\n","      <td>4125</td>\n","      <td>1.131800</td>\n","    </tr>\n","    <tr>\n","      <td>4150</td>\n","      <td>1.397400</td>\n","    </tr>\n","    <tr>\n","      <td>4175</td>\n","      <td>1.104700</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>1.363300</td>\n","    </tr>\n","    <tr>\n","      <td>4225</td>\n","      <td>1.216700</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>1.789800</td>\n","    </tr>\n","    <tr>\n","      <td>4275</td>\n","      <td>1.192300</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>1.257600</td>\n","    </tr>\n","    <tr>\n","      <td>4325</td>\n","      <td>1.212700</td>\n","    </tr>\n","    <tr>\n","      <td>4350</td>\n","      <td>1.450000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=4358, training_loss=1.6439657434513395, metrics={'train_runtime': 6094.0377, 'train_samples_per_second': 2.861, 'train_steps_per_second': 0.715, 'total_flos': 1.217577738338304e+16, 'train_loss': 1.6439657434513395, 'epoch': 1.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Checking GPU compatibility\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Loading base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Loading LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Loading LoRA config\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:36:38.727446Z","iopub.status.busy":"2024-07-07T10:36:38.727053Z","iopub.status.idle":"2024-07-07T10:36:38.957429Z","shell.execute_reply":"2024-07-07T10:36:38.956472Z","shell.execute_reply.started":"2024-07-07T10:36:38.727414Z"},"trusted":true},"outputs":[],"source":["# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:36:40.993347Z","iopub.status.busy":"2024-07-07T10:36:40.992977Z","iopub.status.idle":"2024-07-07T10:36:42.112938Z","shell.execute_reply":"2024-07-07T10:36:42.111720Z","shell.execute_reply.started":"2024-07-07T10:36:40.993307Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","  adding: kaggle/working/results/ (stored 0%)\n","  adding: kaggle/working/results/runs/ (stored 0%)\n","  adding: kaggle/working/results/runs/Jul07_08-34-38_72ee8fb3ce9d/ (stored 0%)\n","  adding: kaggle/working/results/runs/Jul07_08-34-38_72ee8fb3ce9d/events.out.tfevents.1720341295.72ee8fb3ce9d.34.1 (deflated 60%)\n","  adding: kaggle/working/results/runs/Jul07_08-42-21_72ee8fb3ce9d/ (stored 0%)\n","  adding: kaggle/working/results/runs/Jul07_08-42-21_72ee8fb3ce9d/events.out.tfevents.1720341755.72ee8fb3ce9d.34.2 (deflated 68%)\n","  adding: kaggle/working/results/runs/Jul07_08-30-11_72ee8fb3ce9d/ (stored 0%)\n","  adding: kaggle/working/results/runs/Jul07_08-30-11_72ee8fb3ce9d/events.out.tfevents.1720341034.72ee8fb3ce9d.34.0 (deflated 59%)\n"]}],"source":["!zip -r /kaggle/working/llama_math_results_tensorboard.zip /kaggle/working/results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shutil\n","shutil.copy('/content/llama_math_results_tensorboard.zip', '/content/drive/MyDrive/llama_math_results_tensorboard.zip')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:38:08.115461Z","iopub.status.busy":"2024-07-07T10:38:08.114831Z","iopub.status.idle":"2024-07-07T10:38:21.148274Z","shell.execute_reply":"2024-07-07T10:38:21.147057Z","shell.execute_reply.started":"2024-07-07T10:38:08.115421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.59.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n","Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.32.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n","Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"]}],"source":["!pip install tensorboard"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:38:50.992427Z","iopub.status.busy":"2024-07-07T10:38:50.991985Z","iopub.status.idle":"2024-07-07T10:38:51.006225Z","shell.execute_reply":"2024-07-07T10:38:51.005370Z","shell.execute_reply.started":"2024-07-07T10:38:50.992381Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 203), started 0:01:57 ago. (Use '!kill 203' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%load_ext tensorboard\n","%tensorboard --logdir results/runs"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:39:35.937247Z","iopub.status.busy":"2024-07-07T10:39:35.936608Z","iopub.status.idle":"2024-07-07T10:40:41.628453Z","shell.execute_reply":"2024-07-07T10:40:41.627549Z","shell.execute_reply.started":"2024-07-07T10:39:35.937215Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Find the largest number from all natural numbers less than or equal to 100, which become common multiples of 7 and 8 if 2 is subtracted. [/INST] Solution: Let $n$ be the largest number less than or equal to 100 that becomes a common multiple of 7 and 8 when $2$ is subtracted. We can write $n = 7m + 8$ for some integer $m$. Subtracting $2$ from both sides gives $n = 7m + 8 - 2 = 7m + 6$. Since $n$ is less than or equal to 100, $m$ must be less than or equal to 100/7 = 14. Therefore, $n = 7m + 6 = 7(14) + 6 = \\boxed{98}$.</s>\n"]}],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"Find the largest number from all natural numbers less than or equal to 100, which become common multiples of 7 and 8 if 2 is subtracted.\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:43:34.711310Z","iopub.status.busy":"2024-07-07T10:43:34.710568Z","iopub.status.idle":"2024-07-07T10:43:35.578311Z","shell.execute_reply":"2024-07-07T10:43:35.577421Z","shell.execute_reply.started":"2024-07-07T10:43:34.711275Z"},"trusted":true},"outputs":[{"data":{"text/plain":["20772"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-07T10:46:28.271995Z","iopub.status.idle":"2024-07-07T10:46:28.272329Z","shell.execute_reply":"2024-07-07T10:46:28.272178Z","shell.execute_reply.started":"2024-07-07T10:46:28.272164Z"},"trusted":true},"outputs":[],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.push_to_hub(\"utkarshpophli/Llama-2-7b-math-finetune\", check_pr=True)\n","\n","tokenizer.push_to_hub(\"utkarshpophli/Llama-2-7b-math-finetune\",check_pr=True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
